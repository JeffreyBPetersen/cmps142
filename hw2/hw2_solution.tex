\documentclass{article}
\usepackage{amsmath, amssymb, mathtools}
\title{CMPS 142: Homework Assignment 2}
\author{Raymond Colebaugh - 1377877\\Jeffery Petersen - ID\\Peter Czupil - 1317993}
\begin{document}
\maketitle
\begin{enumerate}
        \item 
            \begin{enumerate}
                \item
                    \begin{tabular}{l | c c c c c c}
                        Algorithm      & Correct & Incorrect & MAE & RMS & RAE (\%) & RSE (\%)\\
                        \hline
                        Nearest Neighbor (IB1) & 768 & 0   & 0      & 0      & 0 & 0 \\
                        Naive Bayes            & 586 & 182 & 0.2811 & 0.4133 & 61.8486 & 86.7082 \\
                        Logistic Regression    & 601 & 167 & 0.3063 & 0.3908 & 67.3928 & 81.9907
                    \end{tabular}\\\\
                    Nearest Neighbor is the most accurate algorithm for this dataset. This is perhaps due to 			 the fact that the standard deviation for this particular data set is pretty high which 				 allows for a decision boundary with ample room on either side. 
                \item 
                   Running the Weka logistic regression algortihm on the supplied dataset produced the 				weight vector:
		$$
     			A=\begin{bmatrix}
         				-0.1232 \\
         				-0.0352 \\
        				0.0133 \\
         				-0.0006 \\
				0.0012 \\
				-0.0897 \\
				-0.9452 \\
				-0.0149
       			 \end{bmatrix}
  		$$
		Our bias turned out to be equal to $8.40$. Therefore the equation for our decision 				boundary is: 
		$$\Sigma_i \ a_i * x_i \ + \ 8.40\ = \ 0$$
		By plugging the $x^{(i)}$'s whose prob. distribution is close to $0.50$/$0.50$ into our 				decision boundary equation, we have found that indeed, the points lie very close it.   
	     \\\\
                \item
                    \begin{tabular}{l | c c c c c c}
                        Algorithm      & Correct & Incorrect & MAE & RMS & RAE (\%) & RSE (\%)\\
                        \hline
                        Nearest Neighbor (IB1) & 539 & 229 & 0.2982 & 0.5461 & 65.6046 & 114.5627 \\
                        Naive Bayes            & 586 & 182 & 0.2841 & 0.4168 & 62.5028 & 87.4349 \\
                        Logistic Regression    & 593 & 175 & 0.3094 & 0.3954 & 68.0819 & 82.9651
                    \end{tabular}\\\\
                \item
                   After applying the normalization filter to the dataset, the attributes now have real                		numbered values ranging from $0$ to $1$.
		Running the $10$-fold cross validation yields the same accuracies as well because 					although the attributes now have lesser values, the cumulative value has also decreased 			correspondingly.
		The weight vector changed pretty radically however, before normalization the absolute 			values of the weight vector were very small (between $0$ and $1$). After the 					normalization, the absolute values became much greater (anywhere from $2$ to $8$ on 			average). This is because with classification, the prediction rule depends on how close a 			value is to a $0$ or a $1$. Before the normalization, the attribute values were large and 			thus must be multiplied by a small number to get a number between $0$ and $1$. After 			normalization the roles have reversed thus the weights are large.
                \item
	          When changing the ridge parameter to $0$, the value of the weights don't seem to 				change, neither does the accuracy.
		When changing the ridge parameter to $0.3$ however, the values of the weights 					decreases slighty while the accuracy stays the same. 
                \item
	          We would expect 3NN or 5NN to be more accurate because the feature set is pretty 				large thus many combinations of features that lead to a certain label exist. Therefore 				comparing an instance to more than one of its nearest neighbors reveals more 					combinations than comparing an instance to just one nearest neighbor. The accuracy of 			the prediction when running the ibk algorithm in Weka supports this hypothesis. 3NN 				yields an accuracy of $\%72.6563$ and 5NN gives $\%73.1771$.
                \item
                   We would expect the results to be less accurate when running the classifierSs because 				there is now a lot more random noise. This is not the case with logisitc regression, the 				accuracy actually stays the same. IB1 had a slight drop in accuracy (around $\%1$) with 				folding (using the entire training set as the test set kept an accuracy of $\%100$ for 				obvious reasons) and naive bayes had a much larger drop in accuracy (around $\%8$).
                \item
                   When we ran nearest neighbor on the normalized (features) modified training set, 10 fold 		cross-folding was more inaccurate than nearest neighbor learning from the training sets 			of the previous parts. The accuracy when using the entire training set as the test set was 			$\%100$ however. Naive bayes was  more accurate than the previous two training sets 				and logistic regression was slightly more accurate than the previous training sets as well.   
            \end{enumerate}
        \item
            \begin{enumerate}
                \item The outcome space is the set of possible combinations of whether
                      each of the two children were male or female. Given in the notation
                      of $(younger, older)$, this leads to the outcome space:
                      $$
                        (B, G), (G, B), (B, B), (G, G)
                      $$
                \item We want the probability of at least one child being a girl, given
                      that we already know one child is a boy. We reduce the outcome space
                      by removing the possiblity of $(G, G)$. Then the remaining outcomes
                      are $(B, G), (G, B), (B, B)$. This leaves us with a probability
                      of $\frac{2}{3}$.
                \item Given that we know one child is a boy, then we are left with two
                    cases: either it was the first or the second child.  \\
                    \begin{tabular}{l | r}
                        \begin{tabular}{c c}
                            B & B \\
                            B & G \\
                            -G & B \\
                            -G & G
                        \end{tabular}
                        &
                        \begin{tabular}{c c}
                            B & B \\
                            -B & G \\
                            G & B \\
                            -G & G
                        \end{tabular}
                    \end{tabular} \\
                    This results in two cases where one child is a girl, out of four
                    remaining cases, for a probability of $\frac{1}{2}$.
            \end{enumerate}
        \item
            Given the existing data, we can calculate the mean of the GPA of
            honors students to be $ \frac{4.0 + 3.7 + 2.5}{3} = 3.4 $.
            $$ P( X_{GPA} ) = \frac{1}{\sqrt{2 \pi} 0.6} e^{\frac{(x - 3.4)^2 }{1.2} }$$
            $$ P( X_{AP} ) = \prod\limits_{i = 1}^{n} {P_H}^{x^{(i)}}(1 - P_H)^{1 - x^{(i)}} $$
            Our final prediction is: \\
            If AP courses are taken, predict $H$ if the GPA is between ..., and
            if AP courses are not taken, predict $H$ if the GPA is between ... \\
        \item
            $$ E[V]E[W] = E[VW] \ \ \ (to\ prove)$$
            $$ E[V] = \sum_{i = 1}^{n} v_i P(v_i), E[W] = \sum_{j = 1}^{n} w_i P(w_i) \ \ \ (Definition)$$
            $$ E[VW] = \sum_{i = 1}^{n} v_iw_iP(v_i, w_i) = \sum_{i = 1}^{n} v_iP(v_i)w_iP(v_i) \ \ \ (independence)$$
            $$ = \sum_{i = 1}^{n} v_i P(v_i) \sum_{i = 1}^{n} w_i P(w_i) \ \ \ (Associativity)$$
            $$ = E[V]E[W]\ _\blacksquare$$
\end{enumerate}
\end{document}
