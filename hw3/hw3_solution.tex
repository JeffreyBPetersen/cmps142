\documentclass{article}
\usepackage{amsmath, amssymb, graphicx}
\usepackage[margin=1in]{geometry}
\title{CMPS 142: Homework Assignment 3}
\author{Jeffrey Petersen - 1329242\\Ben LASTNAME- ID\\Raymond Colebaugh - 1377877}
\begin{document}
\maketitle
\begin{enumerate}
        \item 
        \item
            In executing grid search with the specified bounds on parameters, we
            determined the following costs and gammas for SMO:
            $$ \begin{tabular}{c c c c c c c | c c c}
                $C_{ min }$ & $C_{ max }$ & $C_{ step }$ & $\gamma_{ min }$ & $\gamma_{ max }$ & $\gamma_{ step }$ & Sample Size & C & $\gamma$ & Accuracy \\ \hline
                1    &   16    &    1     &       -5     &     2        &        1         &  25\%    &  16 & 0 & 93.6318 \%   \\
                1    &   16    &    1     &       -5     &     2        &        1         &  50\%    &  14 & 1 & 93.6101 \% \\
                15    &   32    &    1     &       0     &     2        &        0.25      &  10\%    &  31 & 0 & 93.6536 \%  \\
                15    &   32    &    1     &       0     &     2        &        0.25      &  25\%    &  25 & 0 & 93.697 \%  \\
                23    &   27    &    0.25  &       0.7   &     1.3      &        0.1       &  10\%    &  27 & 0.7  & 93.3275 \% \\
                23    &   27    &    0.25  &       0.7   &     1.3      &        0.1       &  25\%    &  25.75 & 0.7 & 93.371 \% \\
                23    &   27    &    0.25  &       0.7   &     1.3      &        0.1       &  50\%    &  24.75 & 0.7 & 93.371 \%
            \end{tabular} $$
            Where the expression for $c$ was $i$ and the expression for $\gamma$ was $10^i$.
        \item
            We can demonstrate the construction of a decision tree on a simple
            binary dataset such as:
            $$ \begin{tabular}{c c | c}
                $x_1$ & $x_2$ & $y$ \\ \hline
                0 & 1 & 1 \\
                1 & 1 & 1 \\
                1 & 0 & 0 \\
                0 & 0 & 1 \\
                1 & 0 & 0   % <-
            \end{tabular} $$
            In calculating the decision tree, we first decide the attribute to split
            on for the first node by finding the sum the incorrect predictions. Should we split
            on $x_1$, when $x_1 = 0$ there are no errors, and when $x_1 = 1$ there are
            two errors. Should we split on $x_2$, if $x_2 = 0$ we have one error, and
            if $x_2 = 1$ we have no errors. This makes a split on $x_2$ the better choice.
            Then for the second node, should $x_2 = 0$, we split on the remaining attribute,
            $x_1$. By following decision tree algorithm, we find a representation in the
            following tree:
            $$ \includegraphics[scale=0.5]{3-1.png} $$
            However, there exists a simpler tree to classify this miniscule dataset,
            should we allow compound conditionals on the nodes:
            $$ \includegraphics[scale=0.5]{3-2.png} $$
\end{enumerate}
\end{document}
